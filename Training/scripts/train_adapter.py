#!/usr/bin/env python3
"""
OVEREND Adapter Training Script

ä½¿ç”¨ Apple Foundation Models Adapter Training Toolkit è¨“ç·´
å°ˆå±¬æ–¼å­¸è¡“æ–‡ç»ç®¡ç†çš„ Custom Adapterã€‚

éœ€æ±‚:
- Apple Developer Program entitlement
- Python 3.10+
- PyTorch 2.0+
- Adapter Training Toolkit (å¾ Apple Developer ä¸‹è¼‰)

ä½¿ç”¨æ–¹å¼:
    python train_adapter.py --data ../data/training_data.jsonl
"""

import argparse
import json
from pathlib import Path

# æ³¨æ„: ä»¥ä¸‹ import éœ€è¦å®‰è£ Apple çš„ Adapter Training Toolkit
# from adapter_toolkit import TrainingConfig, Trainer, DataLoader


def load_jsonl(path: str) -> list:
    """è¼‰å…¥ JSONL æ ¼å¼çš„è¨“ç·´è³‡æ–™"""
    data = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def validate_data(data: list) -> bool:
    """é©—è­‰è³‡æ–™æ ¼å¼æ˜¯å¦æ­£ç¢º"""
    for i, item in enumerate(data):
        if 'messages' not in item:
            print(f"éŒ¯èª¤: ç¬¬ {i+1} ç­†è³‡æ–™ç¼ºå°‘ 'messages' æ¬„ä½")
            return False
        
        messages = item['messages']
        if len(messages) < 2:
            print(f"éŒ¯èª¤: ç¬¬ {i+1} ç­†è³‡æ–™éœ€è¦è‡³å°‘ 2 å‰‡è¨Šæ¯ (user + assistant)")
            return False
        
        roles = [m.get('role') for m in messages]
        if 'user' not in roles or 'assistant' not in roles:
            print(f"éŒ¯èª¤: ç¬¬ {i+1} ç­†è³‡æ–™éœ€è¦åŒ…å« 'user' å’Œ 'assistant' è§’è‰²")
            return False
    
    return True


def main():
    parser = argparse.ArgumentParser(description='OVEREND Adapter Training')
    parser.add_argument('--data', type=str, default='../data/training_data.jsonl',
                        help='è¨“ç·´è³‡æ–™è·¯å¾‘ (JSONL æ ¼å¼)')
    parser.add_argument('--output', type=str, default='../adapters/overend_literature.fmadapter',
                        help='è¼¸å‡º Adapter è·¯å¾‘')
    parser.add_argument('--epochs', type=int, default=3,
                        help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='å­¸ç¿’ç‡')
    parser.add_argument('--batch-size', type=int, default=4,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lora-rank', type=int, default=32,
                        help='LoRA rank')
    args = parser.parse_args()
    
    # è¼‰å…¥ä¸¦é©—è­‰è³‡æ–™
    print(f"ğŸ“‚ è¼‰å…¥è¨“ç·´è³‡æ–™: {args.data}")
    data = load_jsonl(args.data)
    print(f"   å…± {len(data)} ç­†è¨“ç·´ç¯„ä¾‹")
    
    if not validate_data(data):
        print("âŒ è³‡æ–™é©—è­‰å¤±æ•—")
        return
    
    print("âœ… è³‡æ–™é©—è­‰é€šé")
    
    # è¨“ç·´é…ç½®
    print(f"\nğŸ“‹ è¨“ç·´é…ç½®:")
    print(f"   Epochs: {args.epochs}")
    print(f"   Learning Rate: {args.lr}")
    print(f"   Batch Size: {args.batch_size}")
    print(f"   LoRA Rank: {args.lora_rank}")
    print(f"   Output: {args.output}")
    
    # TODO: å¯¦éš›è¨“ç·´é‚è¼¯ (éœ€è¦ Apple Adapter Training Toolkit)
    # config = TrainingConfig(
    #     dataset_path=args.data,
    #     output_path=args.output,
    #     epochs=args.epochs,
    #     learning_rate=args.lr,
    #     batch_size=args.batch_size,
    #     lora_rank=args.lora_rank
    # )
    # trainer = Trainer(config)
    # trainer.train()
    
    print("\nâš ï¸  æ³¨æ„: å¯¦éš›è¨“ç·´éœ€è¦ Apple Adapter Training Toolkit")
    print("   è«‹å¾ Apple Developer ä¸‹è¼‰ä¸¦å®‰è£ Toolkit å¾Œå†åŸ·è¡Œè¨“ç·´")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print("\nğŸ‰ æº–å‚™å·¥ä½œå®Œæˆï¼")


if __name__ == '__main__':
    main()
